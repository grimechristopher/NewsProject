{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128772fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Number of GPUs: 1\n",
      "GPU 0:\n",
      "  Name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "  Total memory: 7.66 GB\n",
      "  Current memory usage: 0.00 GB\n",
      "  Memory reserved: 0.00 GB\n",
      "  Memory free: 7.66 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda if torch.cuda.is_available() else \"N/A\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"  Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  Current memory usage: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory free: {(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_reserved(i)) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"No CUDA GPU detected!\")\n",
    "    print(\"Available devices:\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f90d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Delete any existing models/tokenizers\n",
    "try:\n",
    "   del model\n",
    "except:\n",
    "   pass\n",
    "try:\n",
    "   del tokenizer\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Clear Python garbage\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Clear PyTorch VRAM\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Force release reserved memory\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "# Final clear\n",
    "with torch.cuda.device(0):\n",
    "   torch.cuda.empty_cache()\n",
    "   torch.cuda.memory.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is machine learning?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ac8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbbddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "# Load environment and authenticate\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "print(\"Authenticated with HuggingFace\")\n",
    "\n",
    "print(\"Loading large model on CPU...\")\n",
    "\n",
    "# Choose your model - FREE options for 32GB RAM bias analysis:\n",
    "\n",
    "# RECOMMENDED - Safe choices for 32GB RAM with headroom:\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  # BEST - 8B params, 128K context, ~12GB RAM\n",
    "\n",
    "# Other good options (try in this order):\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  # Very safe, still excellent\n",
    "# model_name = \"Qwen/Qwen2.5-14B-Instruct\"                 # Good alternative\n",
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"          # Reliable, well-tested\n",
    "\n",
    "# Only if you want to risk it (might cause OOM):\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"  # Risky on 32GB RAM\n",
    "\n",
    "# Force CPU usage\n",
    "device = \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "print(f\"Loading {model_name} on CPU...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "# Load model on CPU with memory optimizations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "    device_map={\"\": device},    # Force everything to CPU\n",
    "    low_cpu_mem_usage=True,     # Optimize memory usage\n",
    "    token=hf_token,\n",
    "    # Remove quantization config - not needed for CPU\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded on CPU!\")\n",
    "print(f\"Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")\n",
    "\n",
    "# Enhanced prompt specifically for newspaper bias analysis\n",
    "bias_analysis_prompt = \"\"\"You are an expert journalist and media analyst. Analyze the following news articles for bias and rewrite them objectively.\n",
    "\n",
    "Task: \n",
    "1. Identify specific instances of bias (loaded language, selective facts, framing)\n",
    "2. Rewrite the content in a neutral, balanced way\n",
    "3. Ensure all factual claims are presented without editorial slant\n",
    "\n",
    "Articles to analyze:\n",
    "[Insert your newspaper articles here]\n",
    "\n",
    "Analysis and neutral rewrite:\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(bias_analysis_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "\n",
    "print(\"Generating response...\")\n",
    "\n",
    "# Generate with settings optimized for analysis tasks\n",
    "with torch.no_grad():  # Save memory during inference\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.3,      # Lower for more focused analysis\n",
    "        do_sample=True,\n",
    "        top_p=0.9,           # Nucleus sampling for quality\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "print(f\"Response:\\n{response}\")\n",
    "\n",
    "# Memory cleanup\n",
    "del outputs\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ebae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated with HuggingFace\n",
      "Loading model optimized for 16GB RAM...\n",
      "üöÄ GPU detected: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "VRAM available: 8.2GB\n",
      "Loading microsoft/Phi-3-mini-128k-instruct on CUDA...\n",
      "Loading with GPU optimizations for 3070 Ti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [06:54<00:00, 207.16s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model parameters: ~2.0B\n",
      "‚úÖ VRAM usage: 2.4GB allocated, 6.7GB reserved\n",
      "üìä VRAM available: 1.5GB remaining of 8.2GB total\n",
      "‚ö†Ô∏è  Warning: High VRAM usage - monitor for OOM errors\n",
      "\n",
      "==================================================\n",
      "BIAS ANALYSIS SYSTEM READY\n",
      "==================================================\n",
      "\n",
      "To use the bias analyzer:\n",
      "1. Replace sample_articles with your actual article texts\n",
      "2. Run: analysis_result = analyze_articles_for_bias(your_articles)\n",
      "3. The model can handle up to ~50-100 articles at once!\n",
      "\n",
      "üéØ Model ready! Context window: 128K tokens (~400-500 pages)\n",
      "üöÄ GPU acceleration enabled - expect 20-50x faster inference!\n",
      "‚ö° Estimated speed: ~10-30 tokens/second (vs 1-3 on CPU)\n",
      "üî• 3000 token analysis: ~2-5 minutes (vs 50+ minutes on CPU)\n",
      "üí´ Perfect for real-time bias analysis of multiple articles!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load environment and authenticate\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Authenticated with HuggingFace\")\n",
    "\n",
    "print(\"Loading model optimized for 16GB RAM...\")\n",
    "\n",
    "# UPDATED - Models optimized for 7GB VRAM (3070 Ti):\n",
    "# RECOMMENDED for 7GB VRAM + fast inference:\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\" \n",
    "# Other excellent options for 7GB VRAM:\n",
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # 8B params, ~4-5GB VRAM, 128K context  \n",
    "# model_name = \"microsoft/Phi-3-medium-4k-instruct\"  # 14B params, ~6-7GB VRAM, shorter context\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"  # 7B params, ~4GB VRAM, 128K context\n",
    "\n",
    "# If you want to push it (might use 7-8GB):\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"  # 14B params, excellent but tight fit\n",
    "\n",
    "# Models that DON'T fit in 7GB VRAM (commented out):\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"  # 32B - needs 16GB+ VRAM\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"üöÄ GPU detected: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è  No GPU detected, falling back to CPU\")\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(f\"Loading {model_name} on {device.upper()}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "# Load model with GPU optimizations for 7GB VRAM\n",
    "print(\"Loading with GPU optimizations for 3070 Ti...\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float16,    # Essential for GPU - saves memory & faster\n",
    "#     device_map=\"auto\",           # CHANGED: Auto device mapping for GPU\n",
    "#     low_cpu_mem_usage=True,      # Efficient loading\n",
    "#     token=hf_token,\n",
    "#     # GPU-specific optimizations:\n",
    "#     use_safetensors=True,        # More memory efficient loading\n",
    "#     trust_remote_code=True,      # Allow custom model code if needed\n",
    "#     load_in_8bit=False,          # Keep false - float16 is better for your VRAM\n",
    "#     offload_folder=\"./offload\"   # Offload to disk if needed\n",
    "# )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=hf_token,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,\n",
    "    offload_folder=\"./offload\",\n",
    "    max_memory={0: \"3GB\"}  # Much smaller model\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")\n",
    "\n",
    "# Check actual VRAM usage\n",
    "if torch.cuda.is_available():\n",
    "    vram_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    vram_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ VRAM usage: {vram_allocated:.1f}GB allocated, {vram_reserved:.1f}GB reserved\")\n",
    "    print(f\"üìä VRAM available: {vram_total - vram_reserved:.1f}GB remaining of {vram_total:.1f}GB total\")\n",
    "    \n",
    "    # Warn if getting close to limit\n",
    "    if vram_reserved > 6.5:\n",
    "        print(\"‚ö†Ô∏è  Warning: High VRAM usage - monitor for OOM errors\")\n",
    "else:\n",
    "    print(\"Running on CPU - monitor system RAM usage\")\n",
    "\n",
    "# UPDATED - Enhanced bias analysis with full context window\n",
    "def analyze_articles_for_bias(articles, max_tokens=120000):\n",
    "    \"\"\"\n",
    "    Analyze articles for bias using full 128K context window\n",
    "    \n",
    "    Args:\n",
    "        articles: List of article texts or single article text\n",
    "        max_tokens: Maximum tokens to use (leave room for response)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle single article or multiple articles\n",
    "    if isinstance(articles, str):\n",
    "        articles_text = articles\n",
    "        article_count = 1\n",
    "    else:\n",
    "        articles_text = \"\\n\\n--- ARTICLE SEPARATOR ---\\n\\n\".join(articles)\n",
    "        article_count = len(articles)\n",
    "    \n",
    "    # Enhanced bias analysis prompt for multiple articles\n",
    "    bias_analysis_prompt = f\"\"\"<|im_start|>system\n",
    "You are an expert journalist and media analyst with decades of experience identifying bias in news reporting. You specialize in objective analysis and neutral rewriting.<|im_end|>\n",
    "<|im_start|>user\n",
    "Analyze the following {article_count} news article(s) for bias and provide a comprehensive analysis:\n",
    "\n",
    "ANALYSIS REQUIREMENTS:\n",
    "1. **Bias Detection**: Identify specific instances of:\n",
    "   - Loaded/emotional language\n",
    "   - Selective fact presentation\n",
    "   - Framing effects\n",
    "   - Source selection bias\n",
    "   - Omitted context\n",
    "\n",
    "2. **Bias Classification**: For each article, provide:\n",
    "   - Overall bias direction (left/right/center)\n",
    "   - Confidence level (high/medium/low)\n",
    "   - Primary bias techniques used\n",
    "\n",
    "3. **Neutral Rewrite**: Provide a completely objective version that:\n",
    "   - Uses neutral language\n",
    "   - Presents all relevant facts\n",
    "   - Includes necessary context\n",
    "   - Maintains journalistic integrity\n",
    "\n",
    "4. **Cross-Article Analysis** (if multiple articles): \n",
    "   - Compare how different sources frame the same story\n",
    "   - Identify consensus facts vs. disputed interpretations\n",
    "   - Note what each source emphasizes or omits\n",
    "\n",
    "ARTICLES TO ANALYZE:\n",
    "{articles_text}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize with full context window\n",
    "    inputs = tokenizer(\n",
    "        bias_analysis_prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True,\n",
    "        max_length=max_tokens,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "    \n",
    "    # Check if truncation occurred\n",
    "    if 'overflowing_tokens' in inputs and len(inputs['overflowing_tokens']) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Content truncated to fit {max_tokens} tokens\")\n",
    "        print(f\"Using {inputs['length'][0]} tokens out of {len(tokenizer.encode(bias_analysis_prompt))}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Processing {inputs['input_ids'].shape[1]} tokens (fits in context)\")\n",
    "    \n",
    "    print(\"Generating bias analysis...\")\n",
    "    \n",
    "    # Generate with settings optimized for detailed analysis\n",
    "    with torch.no_grad():  # Save memory during inference\n",
    "        outputs = model.generate(\n",
    "            **{k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']},\n",
    "            max_new_tokens=3000,        # INCREASED: More room for detailed analysis\n",
    "            temperature=0.2,            # LOWERED: More focused analysis\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=3      # Reduce repetition\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[-1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Memory cleanup\n",
    "    del outputs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Example usage:\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BIAS ANALYSIS SYSTEM READY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test with sample prompt (replace with your actual articles)\n",
    "sample_articles = [\n",
    "    \"[Insert your first article text here]\",\n",
    "    \"[Insert your second article text here]\"\n",
    "]\n",
    "\n",
    "print(\"\\nTo use the bias analyzer:\")\n",
    "print(\"1. Replace sample_articles with your actual article texts\")\n",
    "print(\"2. Run: analysis_result = analyze_articles_for_bias(your_articles)\")\n",
    "print(\"3. The model can handle up to ~50-100 articles at once!\")\n",
    "\n",
    "# Uncomment to run analysis:\n",
    "# analysis_result = analyze_articles_for_bias(sample_articles)\n",
    "# print(f\"\\nBias Analysis Result:\\n{analysis_result}\")\n",
    "\n",
    "print(f\"\\nüéØ Model ready! Context window: 128K tokens (~400-500 pages)\")\n",
    "print(f\"üöÄ GPU acceleration enabled - expect 20-50x faster inference!\")\n",
    "print(f\"‚ö° Estimated speed: ~10-30 tokens/second (vs 1-3 on CPU)\")\n",
    "print(f\"üî• 3000 token analysis: ~2-5 minutes (vs 50+ minutes on CPU)\")\n",
    "print(f\"üí´ Perfect for real-time bias analysis of multiple articles!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44957bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"\n",
    "Skip to main content\n",
    "\n",
    "Newsletters\n",
    "Axios Pro\n",
    "\n",
    "    Axios Live\n",
    "\n",
    "Axios\n",
    "Updated 8 hours ago -\n",
    "Politics & Policy\n",
    "House bails early for its August recess amid Epstein files uproar\n",
    "\n",
    "    Kate Santaliz,\n",
    "    Andrew Solender\n",
    "\n",
    "Majority Leader Steve Scalise, Speaker Mike Johnson and Conference Chair Rep. Lisa McClain, during an enrollment ceremony for H.R. 1 on July 03. Photo by Kevin Dietsch/Getty Images\n",
    "\n",
    "House Speaker Mike Johnson, conference chair Lisa McClain, and Majority Leader Steve Scalise during an enrollment ceremony on July 3. Photo: Kevin Dietsch/Getty Images\n",
    "\n",
    "The House is leaving Washington a day early for its five-week August recess after tensions erupted over efforts to force release of the Jeffrey Epstein files.\n",
    "\n",
    "Why it matters: Fallout from the debate over the Epstein files has effectively frozen House business. Leadership opted to cut the week short and leave town before things escalate further.\n",
    "\n",
    "    The final House votes before September are now scheduled for Wednesday afternoon.\n",
    "\n",
    "State of play: The House Rules Committee is not planning to hold votes this week to prepare major legislation for the House floor, meaning any remaining votes will likely be on small, noncontroversial bills.\n",
    "\n",
    "    Democrats planned to force yet more votes on amendments aimed at pressuring the Justice Department to release all its documents on Epstein.\n",
    "    The House had been scheduled to vote on GOP legislation involving immigration and environmental policies this week, which had to go through the Rules Committee first.\n",
    "\n",
    "What they're saying: Speaker Mike Johnson (R-La.) insisted Monday night that the House would continue its work \"all week.\"\n",
    "\n",
    "    \"We have lots of work with appropriations and a lot of committees doing very important stuff,\" Johnson said late Monday evening.\n",
    "    But by Tuesday morning, Majority Whip Tom Emmer (R-Minn.) officially noticed the change in schedule.\n",
    "    Johnson said at a press conference Tuesday that Republicans were \"done being lectured about transparency\" and that he would not allow Democrats to \"continue with their nonsense this week.\" \n",
    "\n",
    "The other side: Democrats used the canceled votes as another opportunity to claim that Republicans are trying to stifle attempts to get them on the record about the Epstein files.\n",
    "\n",
    "    \"They're scared sh--tless!\" gloated Rep. Jim McGovern (D-Mass.), who has repeatedly forced votes in his panel on releasing the DOJ's Epstein documents.\n",
    "\n",
    "The big picture: While GOP leadership continues to assert that there's \"no daylight\" between the House and the Trump administration over the Epstein files, frustration inside GOP ranks is growing.\n",
    "\n",
    "    Several rank-and-file Republicans are pressuring leadership to take up a nonbinding resolution demanding full disclosure from the DOJ before the August recess.\n",
    "    Johnson has resisted, saying the White House needs \"space\" to act on its own.\n",
    "    Rep. Thomas Massie (R-Ky.) is pushing forward with a discharge petition to force a vote on releasing the files, directly challenging party leaders.\n",
    "\n",
    "What's next: Republicans on the House Oversight Committee also moved to subpoena Epstein associate Ghislaine Maxwell on Tuesday ‚Äî and said they did not consult the White House or GOP leadership before doing so.\n",
    "\n",
    "Editor's note: This story has been updated with additional reporting.\n",
    "\n",
    "Go deeper\n",
    "\n",
    "    Andrew Solender,\n",
    "    Kate Santaliz\n",
    "\n",
    "Updated 23 hours ago -\n",
    "Politics & Policy\n",
    "House grinds to a halt as GOP tries to shut down Epstein votes\n",
    "Close-up of a middle-aged man speaking, wearing a dark suit and red tie, with a blurred American flag background.\n",
    "\n",
    "House Majority Leader Steve Scalise speaks at a press conference at the Republican National Committee headquarters on July 15. Photo: Kevin Dietsch/Getty Images\n",
    "\n",
    "House Republicans have virtually stopped work on all major legislation leading up to their six-week summer recess to avoid taking votes on forcing the release of the Jeffrey Epstein files.\n",
    "\n",
    "Why it matters: Democrats have been using every opportunity to force their GOP colleagues on the record about Epstein as President Trump pressures them to make the issue go away.\n",
    "Go deeper (2 min. read)\n",
    "\n",
    "    Andrew Solender,\n",
    "    Kate Santaliz\n",
    "\n",
    "Jul 18, 2025 -\n",
    "Politics & Policy\n",
    "Inside the GOP-led plot to defy Trump on the Epstein files\n",
    "Rep. Thomas Massie, wearing a blue suit and standing in a white and yellow hallway.\n",
    "\n",
    "Rep. Thomas Massie at the U.S. Capitol on July 2. Photo: Kent Nishimura/Bloomberg via Getty Images\n",
    "\n",
    "Rep. Thomas Massie (R-Ky.) is moving forward with plans to force a vote on requiring the release of the Jeffrey Epstein files, despite attempts by President Trump and House Speaker Mike Johnson (R-La.) to dampen his efforts.\n",
    "\n",
    "Why it matters: The push by Massie and Rep. Ro Khanna (D-Calif.) has proven popular in Congress, with most Democrats and some on the GOP's right flank supporting it.\n",
    "Go deeper (2 min. read)\n",
    "\n",
    "    Andrew Solender,\n",
    "    Kate Santaliz\n",
    "\n",
    "Jul 15, 2025 -\n",
    "Politics & Policy\n",
    "House GOP blocks second Dem attempt to release Epstein files\n",
    "House Speaker Mike Johnson speaking at a podium with a red sign while holding up a pen, flanked by GOP colleagues with an American flag behind him.\n",
    "\n",
    "House Speaker Mike Johnson holds a press conference at the Republican National Committee headquarters in Washington on July 15. Photo: Tom Williams/CQ-Roll Call, Inc via Getty Images\n",
    "\n",
    "House Republicans on Tuesday voted down another Democratic procedural maneuver aimed at forcing the Justice Department to release documents related to Jeffrey Epstein.\n",
    "\n",
    "Why it matters: It's the second time this week Democrats have forced their GOP colleagues to choose between loyalty to President Trump and a MAGA base that is furious at his administration over its handling of the Epstein files.\n",
    "Go deeper (1 min. read)\n",
    "\n",
    "Smarter, faster on what matters.\n",
    "Explore Axios Newsletters\n",
    "\n",
    "    About Axios\n",
    "    Advertise with us\n",
    "    Careers\n",
    "    Contact us\n",
    "\n",
    "    Newsletters\n",
    "    Axios Live\n",
    "    Axios HQ\n",
    "\n",
    "    Privacy policy\n",
    "    Terms of use\n",
    "\n",
    "Axios Homepage\n",
    "\n",
    "Axios Media Inc., 2025\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbb9bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated with HuggingFace\n",
      "Loading Phi-3 with 128k context...\n",
      "üöÄ GPU detected: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Loading microsoft/Phi-3-mini-128k-instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Load model with proper settings for Phi-3\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use bfloat16 instead of float16\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use flash attention\u001b[39;49;00m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 8bit instead of 4bit for stability\u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m6GB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Leave some VRAM headroom\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/llm-sandbox/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:593\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m         model_class.register_for_auto_class(auto_class=\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    592\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    597\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/llm-sandbox/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/llm-sandbox/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4758\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4756\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4757\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m4758\u001b[39m     config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4764\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4765\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m   4766\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, *model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/llm-sandbox/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2315\u001b[39m, in \u001b[36mPreTrainedModel._autoset_attn_implementation\u001b[39m\u001b[34m(cls, config, torch_dtype, device_map, check_device_map)\u001b[39m\n\u001b[32m   2307\u001b[39m     \u001b[38;5;28mcls\u001b[39m._check_and_enable_flash_attn_3(\n\u001b[32m   2308\u001b[39m         config,\n\u001b[32m   2309\u001b[39m         torch_dtype=torch_dtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2312\u001b[39m         check_device_map=check_device_map,\n\u001b[32m   2313\u001b[39m     )\n\u001b[32m   2314\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2315\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2322\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflex_attention\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2323\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._check_and_enable_flex_attn(config, hard_check_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/llm-sandbox/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2457\u001b[39m, in \u001b[36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[39m\u001b[34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[39m\n\u001b[32m   2455\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[32m   2456\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2457\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2459\u001b[39m flash_attention_version = version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mflash_attn\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   2460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.version.cuda:\n",
      "\u001b[31mImportError\u001b[39m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load environment and authenticate\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Authenticated with HuggingFace\")\n",
    "\n",
    "print(\"Loading Phi-3 with 128k context...\")\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"üöÄ GPU detected: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è No GPU detected, falling back to CPU\")\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token, trust_remote_code=True)\n",
    "\n",
    "# Load model with proper settings for Phi-3\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 instead of float16\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=hf_token,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # Use flash attention\n",
    "    load_in_8bit=True,  # Use 8bit instead of 4bit for stability\n",
    "    max_memory={0: \"6GB\"}  # Leave some VRAM headroom\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "def extract_article(raw_text):\n",
    "    \"\"\"Extract article text using Phi-3 with 128k context\"\"\"\n",
    "    \n",
    "    # Use proper Phi-3 chat format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at extracting clean article text from raw HTML/webpage content.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Extract only the main article text from this webpage content, removing all ads, navigation, headers, footers, and other non-article elements:\\n\\n{raw_text}\"}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=120000)  # Use most of 128k\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "print(\"Phi-3 ready with 128k context!\")\n",
    "print(\"Use: clean_article = extract_article(raw_text)\")\n",
    "\n",
    "clean_article = extract_article(raw_text)\n",
    "print(f\"Extracted article text:\\n{clean_article}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
